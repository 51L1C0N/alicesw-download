import requests
from bs4 import BeautifulSoup
import time
import random
import json
import os
import re
import sys
from urllib.parse import urljoin

# ================= ğŸ›ï¸ ç”¨æˆ¶ä¸­å¤®é…ç½®å€ (User Config) =================

# [1. æª”æ¡ˆèˆ‡ç›®æ¨™]
NOVEL_NAME = "æˆ‘çš„å°èªªä¸‹è¼‰"        # å­˜æª”æª”å
COOKIE_FILE = "cookie.json"      # Cookie æª”æ¡ˆè·¯å¾‘

# [2. è¡Œç‚ºæ§åˆ¶]
USE_COOKIES = True               # æ˜¯å¦æ›è¼‰ Cookie
USER_AGENT_TYPE = "PC"           # "PC" æˆ– "MOBILE"
ENABLE_SMART_FORMAT = True       # æ˜¯å¦å•Ÿç”¨æ™ºæ…§æ’ç‰ˆæ¸…æ´—
SKIP_EXISTING = True             # æ–·é»çºŒå‚³ï¼šè·³éå·²ä¸‹è¼‰çš„ç« ç¯€

# [3. ç¶²çµ¡é˜²è­· (åçˆ¬èŸ²æ ¸å¿ƒ)]
DELAY_RANGE = (3, 6)             # æ­£å¸¸éš¨æ©Ÿå»¶é² (ç§’)
MAX_RETRIES = 20                 # å–®ç« æœ€å¤§é‡è©¦æ¬¡æ•¸ (-1 ç‚ºç„¡é™)
RETRY_CYCLE = [5, 10, 30, 60]    # å¾ªç’°é€€é¿ç­–ç•¥ (ç§’)

# ===================================================================

class ScraperEngine:
    def __init__(self, plugin):
        self.plugin = plugin
        self.headers = self._get_headers()
        self.existing_chapters = set()
        self.session = requests.Session() # ä½¿ç”¨ Session ä¿æŒé€£ç·š

    def _get_headers(self):
        """çµ„è£ Headers"""
        if USER_AGENT_TYPE == "MOBILE":
            ua = "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"
        else:
            ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"

        headers = {"User-Agent": ua}

        # è¼‰å…¥ Cookie
        if USE_COOKIES and os.path.exists(COOKIE_FILE):
            try:
                with open(COOKIE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # å…¼å®¹ Cookie-Editor çš„ JSON æ ¼å¼
                if isinstance(data, list):
                    cookie_str = "; ".join([f"{i['name']}={i['value']}" for i in data if 'name' in i])
                    headers["Cookie"] = cookie_str
                    print("âœ… Cookie è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ Cookie è®€å–å¤±æ•—: {e} (å°‡ä»¥éŠå®¢èº«ä»½è¨ªå•)")
        return headers

    def _smart_request(self, url):
        """
        æ ¸å¿ƒè«‹æ±‚å‡½æ•¸ï¼šåŒ…å«ã€Œå¾ªç’°æŒ‡æ•¸é€€é¿ã€æ©Ÿåˆ¶
        """
        retry_idx = 0
        while True:
            try:
                resp = self.session.get(url, headers=self.headers, timeout=20)

                # è‡ªå‹•è™•ç†ç·¨ç¢¼
                if resp.encoding == 'ISO-8859-1':
                    resp.encoding = resp.apparent_encoding

                if resp.status_code == 200:
                    return BeautifulSoup(resp.text, 'html.parser')
                elif resp.status_code == 404:
                    print(f"âŒ 404 é é¢ä¸å­˜åœ¨: {url}")
                    return None
                elif resp.status_code == 403:
                    print(f"ğŸš« 403 ç¦æ­¢è¨ªå• (å¯èƒ½éœ€è¦æ›´æ–° Cookie)")
                    raise Exception("403 Forbidden")
                else:
                    raise Exception(f"Status {resp.status_code}")

            except Exception as e:
                # é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸
                if MAX_RETRIES != -1 and retry_idx >= MAX_RETRIES:
                    print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„æ­¤ç« ã€‚")
                    return None

                # è¨ˆç®—ç­‰å¾…æ™‚é–“ (å¾ªç’°ç­–ç•¥)
                wait_time = RETRY_CYCLE[retry_idx % len(RETRY_CYCLE)]
                print(f"âš ï¸ è«‹æ±‚å¤±æ•—: {e}")
                print(f"â³ è§¸ç™¼é€€é¿æ©Ÿåˆ¶: ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦... (ç¬¬ {retry_idx+1} æ¬¡)")

                time.sleep(wait_time)
                retry_idx += 1

    def _clean_text(self, text):
        """æ™ºæ…§æ’ç‰ˆæ¸…æ´—"""
        if not ENABLE_SMART_FORMAT:
            return text

        # 1. å»é™¤å¹²æ“¾ç¢¼
        text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', text)

        # 2. æ™ºæ…§åˆä½µæ–·è¡Œ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        cleaned_lines = []
        buffer = ""

        for line in lines:
            if not buffer:
                buffer = line
            else:
                last_char = buffer[-1]
                first_char = line[0]
                # åˆ¤æ–·é‚è¼¯ï¼šçµå°¾éæ¨™é» + é–‹é ­éå¼•è™Ÿ = æ‡‰åˆä½µ
                is_end = last_char in "ã€‚ï¼ï¼Ÿ!?â€¦ã€â€"
                is_start = first_char in "ã€[(ã€Œâ€œ"

                if not is_end and not is_start:
                    buffer += line
                else:
                    cleaned_lines.append(buffer)
                    buffer = line
        if buffer: cleaned_lines.append(buffer)

        return "\n\n".join(cleaned_lines)

    def _load_existing_chapters(self, filename):
        """æ–·é»çºŒå‚³ï¼šè®€å–å·²å­˜åœ¨çš„ç« ç¯€æ¨™é¡Œ"""
        if not os.path.exists(filename):
            return
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                content = f.read()
            # å‡è¨­ç« ç¯€æ¨™é¡Œè¢« === åŒ…åœ
            titles = re.findall(r'={20}\n(.*?)\n={20}', content)
            self.existing_chapters = set(t.strip() for t in titles)
            print(f"ğŸ“‚ æª¢æ¸¬åˆ°å·²ä¸‹è¼‰ {len(self.existing_chapters)} ç« ï¼Œå°‡è‡ªå‹•è·³éã€‚")
        except Exception:
            pass

    def run(self):
        print("ğŸš€ å•Ÿå‹•é€šç”¨æ¡é›†å¼•æ“...")

        # 1. ç²å–ç›®éŒ„
        catalog_url = self.plugin.CATALOG_URL
        print(f"ğŸ” æ­£åœ¨è§£æç›®éŒ„: {catalog_url}")

        soup = self._smart_request(catalog_url)
        if not soup:
            print("âŒ ç„¡æ³•è®€å–ç›®éŒ„ï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
            return

        # èª¿ç”¨æ’ä»¶è§£æç›®éŒ„
        chapters = self.plugin.parse_catalog(soup, catalog_url)

        if not chapters:
            print("âŒ æ‰¾ä¸åˆ°ä»»ä½•ç« ç¯€ï¼Œè«‹æª¢æŸ¥æ’ä»¶é¸æ“‡å™¨ã€‚")
            return

        # è™•ç†å€’åº (å¦‚æœæ’ä»¶æŒ‡å®šäº† REVERSE=True)
        if getattr(self.plugin, 'REVERSE_ORDER', False):
            print("ğŸ”„ åŸ·è¡Œå€’åºæ’åˆ—...")
            chapters.reverse()

        total = len(chapters)
        print(f"ğŸ“– ç™¼ç¾ {total} ç« ï¼Œæº–å‚™ä¸‹è¼‰...")

        # 2. æº–å‚™å­˜æª”
        filename = f"{NOVEL_NAME}.txt"
        if SKIP_EXISTING:
            self._load_existing_chapters(filename)

        with open(filename, "a", encoding="utf-8") as f:
            for index, (title, url) in enumerate(chapters):
                # æ–·é»çºŒå‚³æª¢æŸ¥
                if SKIP_EXISTING and title in self.existing_chapters:
                    print(f"â© [è·³é] {title} (å·²å­˜åœ¨)")
                    continue

                print(f"â¬‡ï¸ [{index+1}/{total}] ä¸‹è¼‰: {title}")

                # è«‹æ±‚ç« ç¯€å…§å®¹
                page_soup = self._smart_request(url)
                if page_soup:
                    # èª¿ç”¨æ’ä»¶è§£æå…§æ–‡
                    raw_text = self.plugin.parse_content(page_soup)

                    if raw_text:
                        final_text = self._clean_text(raw_text)

                        # å¯«å…¥
                        f.write(f"\n\n{'='*20}\n{title}\n{'='*20}\n\n")
                        f.write(final_text)
                        f.flush() # å¼·åˆ¶å­˜æª”
                    else:
                        print(f"   âš ï¸ å…§å®¹è§£æç‚ºç©º")
                        f.write(f"\n\n[ç« ç¯€ {title} è®€å–å¤±æ•—]\n\n")

                # æ­£å¸¸éš¨æ©Ÿå»¶é²
                delay = random.uniform(*DELAY_RANGE)
                print(f"   ğŸ’¤ ä¼‘æ¯ {delay:.1f} ç§’...")
                time.sleep(delay)

        print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼æª”æ¡ˆ: {filename}")

if __name__ == "__main__":
    print("âš ï¸ è«‹ä¸è¦ç›´æ¥é‹è¡Œæ­¤æª”æ¡ˆï¼Œè«‹é‹è¡Œ `run_task.py`")
