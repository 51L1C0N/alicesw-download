from urllib.parse import urljoin

# ================= ğŸ”Œ æ’ä»¶é…ç½®å€ =================
# è«‹å¡«å…¥æ‚¨è¦ä¸‹è¼‰çš„å°èªªç›®éŒ„ç¶²å€
CATALOG_URL = "https://www.alicesw.com/other/chapters/id/49606.html"

# ç¶²ç«™ç‰¹æ€§
REVERSE_ORDER = False   # æ­£åº
NEED_LOGIN = True       # éœ€è¦ Cookie

def parse_catalog(soup, base_url):
    """
    è§£æç›®éŒ„é ï¼šå¾©åˆ»åŸç‰ˆé‚è¼¯ï¼Œæœå°‹æ‰€æœ‰å« /book/ çš„é€£çµ
    """
    chapters = []

    # åŸç‰ˆé‚è¼¯ï¼šæŠ“å–æ‰€æœ‰é€£çµï¼Œé€éé—œéµå­—éæ¿¾
    links = soup.select("a")

    seen = set()
    for link in links:
        title = link.get_text().strip()
        href = link.get('href')

        # éæ¿¾æ¢ä»¶ï¼š
        # 1. é€£çµå¿…é ˆåŒ…å« '/book/' (é€™æ˜¯é–±è®€é ç‰¹å¾µ)
        # 2. æ¨™é¡Œé•·åº¦ > 1
        if href and "/book/" in href and len(title) > 1:
            full_url = urljoin(base_url, href)

            if full_url not in seen:
                chapters.append((title, full_url))
                seen.add(full_url)

    return chapters

def parse_content(soup):
    """
    è§£æå…§æ–‡é ï¼šå¾©åˆ»åŸç‰ˆé‚è¼¯
    """
    # å˜—è©¦å¤šç¨®å¸¸è¦‹ ID/Class
    content = soup.select_one("#content") or \
              soup.select_one(".read-content") or \
              soup.select_one(".chapter-content") or \
              soup.select_one(".novelcontent")

    if content:
        # æ¸…æ´—ï¼šç§»é™¤å¤šé¤˜æ¨™ç±¤ (ä¿ç•™æ’ç‰ˆçµæ§‹)
        for trash in content(["script", "style", "div", "iframe", "a", "ins"]):
            trash.decompose()

        # é—œéµï¼šä½¿ç”¨ "\n\n" ä½œç‚ºåˆ†éš”ç¬¦ï¼Œé€™æ¨£ HTML è£¡çš„æ›è¡Œ/æ®µè½æœƒè¢«ä¿ç•™
        # é€™æ¯”ä¸»ç¨‹åºçš„ Smart Format æ›´é©åˆæ­£è¦ç¶²ç«™
        return content.get_text("\n\n", strip=True)

    return None
